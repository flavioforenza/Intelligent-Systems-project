\section{A Data Set for Camera-Independent Color Constancy}

\begin{flushleft}
    \author{
    Ã‡a$ \breve{g} $lar Aytekin, 
    Jarno Nikkanen, 
    Moncef Gabbouj
    \emph{Fellow, IEEE}
    }
\end{flushleft}

\begin{center}
    \emph{IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 27, NO. 2, FEBRUARY 2018}
\end{center}

\subsection{INTRODUCTION}
Color constancy is a characteristic of the human visual system (HVS) that 
helps to perceive a constant color, for example of an object, at different levels 
of illuminations. It is claimed that the achievement of constancy occurs by 
approximating the composition of the lighting in order to obtain the true 
color of the object. There are supervised and unsupervised methods 
that calculate color consistency. Unsupervised methods are divided into two 
categories, based on the techniques they use to estimate the color of 
the illuminating source. The first category makes statistical assumptions 
about reflectance in a scene. The second category instead uses the physical 
properties of objects in scenes. Supervised methods also fall into two categories. 
The first category tries to learn a combination of unsupervised methods to 
estimate the illumination. The second category builds its own model for 
learning about illumination. The major factor affecting all of these methods 
is the sensitivity of the camera sensor. When the sets used for training, 
validation and testing contain images taken by different cameras, the results 
returned by the algorithms may be different, as well as their performance. 
On the other hand, one method returns "fixed" results when operating on 
images from the same camera. In the CC field, this problem is called camera-independence.
This report provides a dataset, called \emph{Intel-TUT}, which is 
useful for testing camera-independence in the CC. Three different cameras 
capture real scenes both in the lab and elsewhere. Laboratory images have 
different lighting conditions. The dataset contains 1536 images and a test 
set consisting of 454 images taken by a single camera.

\subsection{COLOR CONSTANCY DATASETS}
One of the first datasets for calculating the CC was the one proposed in 
\cite{0807099130}. The single camera captured images showing a total of 1995 surfaces, 
11 lab illuminations and 81 illuminations from the real world. The dataset 
also contains a number of images that make up 50 scenes. After a correct 
calibration, it removes the irrelevant images, the remainder was made up of 
only 529 images which made up 30 scenes. Each of these images belongs to 
the set captured in the lab. Another dataset is the one proposed in \cite{0807099132}, consisting 
of 11,000 images of indoor / outdoor scenes. The scenes were captured in 
different geographic locations and under different weather conditions. Unfortunately 
this dataset contains low resolution images that require a correction 
phase. Another dataset containing 246 indoor images and 322 outdoor images, 
taken by two different cameras, is the one proposed in \cite{0807099120}. Another 
dataset that uses the auto-bracketing technique to acquire the images is the 
one proposed in \cite{0807099134}. In this dataset they are acquired up to 9 different 
images, of the same scene, with different settings for each shot. A dataset that 
aims to obtain multiple lighting in a scene is the one proposed in \cite{0807099135}. 
This contains relatively few images, 9 acquired in an outdoor environment 
and 59 lab images. The is also a dataset containing the videos mentioned 
in \cite{0807099136}. There was only one study \cite{0807099129} that relies on color conversion in order 
to achieve camera independence. However, this method requires a very 
sensitive spectral camera to achieve good performance, so it is not applicable 
to all images. Finally, a last dataset consisting of 1600 indoor and outdoor 
images, taken by 9 different cameras, is the one proposed in \cite{0807099128} called NUS. 
In this collection, each scene was captured by each camera with slight misalignments. 
The database proposed in this paper is similar to the NUT only 
for the different cameras used. However, the proposed database has some 
features such as the following:
\begin{itemize}
    \item Provides the spectral sensitivities of the cameras;
    \item Different illuminants illuminate the scene;
    \item Among the different cameras used, one is mobile;
    \item Provides a set of tests for good evaluation of CC methods.
\end{itemize}

\subsection{THE PROPOSED INTEL-TUT DATA SET}
The purpose of the dataset is to help the algorithms to determine if the color 
constancy value obtained is to be considered good or not. If an algorithm 
does not find a change in this value on the same set of images taken by the 
same camera, then this algorithm is able to have a uniform color constancy 
and consequently would produce optimal outputs.

\subsubsection{Light Sources}
A variety of devices were used that were capable of producing a light source. 
Each property such as luminance (Lux), color temperature (CCT) and CIE 
xy chromaticity, have been appropriately set to have an equitable acquisition.

\subsubsection{Cameras}
Three types of camera are used for the construction of the dataset. Two 
of these belong to the high-end, while a third belongs to the category of 
mobile. To achieve correct color correction, several 3x3 size color 
conversion matrices (\emph{CCMs}) were used. In other words, try to transform 
the components of the image from RGB to sRGB in order to adapt them to 
the lighting under which the image is viewed. For the outdoors, 10 CCMs 
were used based on the type of illumination. Another factor that required 
correction was the color shading (\emph{CS}). This only requires correction for the 
images coming from the mobile.

\subsubsection{Scene Contents}
The scenes that make up the dataset are divided into lab scenes and field 
scenes. There are several types of different lab scenes, each having 5 different 
illuminations (Fig. \ref{fig:Lab}). Both the lab scenes and those acquired in the field 
are 64, for a total of 128 scenes. Each lab scene is captured with each camera 
in almost the same shot. An attempt was made to preserve the same settings 
in the field scenes even if the latter have different lighting. In order to have 
good stabilization, a fixed tripod is used.
\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.6 \linewidth]{images/paper4/lab.png}
    \centering
    \caption{Lab real scenes}
    \label{fig:Lab}
\end{figure}

\subsubsection{Ground Truths}
The ground-truth is based on the chromaticity of the illumination, or white 
points, assumed by the camera which can change for each image. This parameter 
is calculated by averaging the achromatic colors present in the lighter 
patches of the color-checker, that appear in the last line.

\subsubsection{Recommendations for Evaluating CC Algorithm Performance}
In order to evaluate the performance of the algorithm with this type of 
dataset, it is recommended to use the following strategies:
\begin{enumerate}
    \item \emph{Camera independence}: the article proposes six-fold cross validation, 
    where each fold, divided into training, validation and test, contains a 
    grouping of images for each of the nine cameras. This technique aims 
    to obtain a fair evaluation of camera-independence of an algorithm (the 
    algorithm considers all images as if coming from the same camera).
    \item \emph{Camera and Scene Independence}: in this case it is recommended to 
    use the images acquired by Nikon and the mobile for training, while 
    the Canon images must be used for validation. As for the test, it is 
    recommended to use the Canon images belonging to the second group 
    of images. In this way you can understand the influence of camera-
    independence. If you get a low error, which means that the algorithm 
    treats each scene (each made up of images captured by different cameras) 
    as if it had been captured entirely by a single camera, then at 
    this point you would get, in addition to independence of the camera, 
    the independence of the scene. This is a good result if it is achieved in 
    this type of dataset.
    \item \emph{Camera and Scene Independence from Single Camera}: in this strategy 
    it is suggested to use the images from the second group as training 
    and validation while using the images acquired by Nikon and the 
    mobile for testing. In this way we want to understand if the algorithm 
    obtains the camera/scene independence effect with images produced 
    by another different camera (Canon). If this technique were 
    used with the 1 (where a low error implies the achievement of the 
    camera-independence) then when new images of different origin would 
    be inserted, the error would be higher than that obtained in 1, this is 
    because the camera-independence it faces the old set of cameras and not 
    the new one. It is being said that thanks to the combination of these, 
    it is possible to have a targeted camera-independence and a targeted 
    scene-independence for each new camera.
    \item \emph{Testing the Effect of Color Shading}: if you were to use the images 
    of your mobile, to test their color shading correction (CS), you could 
    enclose them all in a test set. Images that cause a high error will be 
    those that have not undergone the correction process.
    \item \emph{Testing the Effect of Resolution}: color constancy can be easily calculated 
    when the images are at high resolution. As you can guess, this 
    result is not achievable with images from a mobile phone. For this 
    reason, the dataset contains a portion of image downscaling (1080p) 
    useful for observing how the resolution can affect the performance of 
    the algorithm.
\end{enumerate}

\subsection{EXPERIMENTAL RESULTS}
\subsubsection{Evaluation of Unsupervised Baseline Methods}
In summary, in order to have the effect of the camera-independecy on a 
set of images from different cameras, it is enough to be able to calculate 
the constancy of the color which, in other words, means having to detect the 
chromaticity of the illuminate that precisely illuminates the scene. Given the 
chromaticity estimated by the algorithm ($ p^{Est} $) and the effective chromaticity 
(white point or ground truth) ($ p^E $), the performance of an algorithm can be 
evaluated based on the result returned by the recovery angular error (RAE):
\begin{equation}
    RAE=\cos^{-1}\left(\frac{p^Ep^{Est}}{||p^E||~||p^{Est}||}\right)
\end{equation}
This error index is calculated both on the performances obtained in high 
resolution images and in those obtained at low resolution. To make some 
comparison, an algorithm that has a low RAE is the unsupervised algorithm 
GW \cite{0807099104} which assumes that the average chromaticity in an image is gray. 
Even the MaxRGB algorithm \cite{0807099103} manages to have good performance 
in images where there are saturated areas where, thanks to these, the white 
point can be easily recovered. MaxRGB is currently the only algorithm that 
manages to have a low RAE even with low quality images (1080p). The 
performances of these algorithms, together with two others (SoG and GE), 
are compared in figure \ref{fig:RAE} where for each set of images (Lab 
Printouts, Lab Real Scenes, $ 1^{st} $ Field Set and $ 2^{nd} $ Field set), taken from different cameras, 
are calculated mean, median and maximum of RAE values reached for each 
image. Subsequently, in figure \ref{fig:RAEstandard} the mean, median and maximum of the 
standard deviation obtained from each image, from each algorithm, are calculated 
again. As you can see, the GW algorithm is the only one to have the 
camera-independence for all the rooms used.

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 1 \linewidth]{images/paper4/RAEHigh.png}
    \centering
    \caption{Recovery Angular Errors (Mean, Median and Maximum) of some algorithms.}
    \label{fig:RAE}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.6 \linewidth]{images/paper4/standardHigh.png}
    \centering
    \caption{Mean, Median and Maximum RAE of some algorithms.}
    \label{fig:RAEstandard}
\end{figure}

\subsubsection{Evaluation of a Direct Supervised Method}
