\section{Variational-Based Mixed Noise Removal With CNN Deep Learning Regularization}

\begin{center}
    \author{
    Faqiang Wang,
    Haiyang Huang,
    Jun Liu
    }
\end{center}

\begin{center}
    \emph{IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.29, 2020}
\end{center}

\subsection{INTRODUCTION}
Random noise distributions correspond to standard probabilistic distributions, 
such as the Gaussian, Poisson and other distributions. There have 
been many methods that attempt to clean up noisy images. The classical 
methods are based on the use of filters such as Gaussian, Gabor or median 
filters. Other tools such as the Variational method have been widely used. 
This method is based on reducing the cost function containing the fidelity 
term of the data, useful for calculating the difference between true and observed 
data, and the regularization terms. Another method used was that of 
TV regularization. Unfortunately this method fails to preserve texture detail. 
Nonlocal methods, on the other hand, are able to capture image details. 
These methods exploit the self-similarity properties that can be integrated 
into variational methods. Despite these methods, the function of the weights 
is difficult to determine. Methods based on deep learning have achieved good 
noise reduction performance. Many of these assume that the value can be 
removed by applying the L2 norm. According to the study carried out in 
the following article, in order to remove the mixed noise it is necessary to 
determine the type and level of noise in each pixel. The methods based on 
the variation, unlike the neural networks, require a single image and can 
integrate different techniques of regularization useful to be able to remove 
the single noise and the mixed noise. The complexity of these methods lies 
in being able to create a non-computationally complicated design. In the 
proposed article, the EM (Expectation-Maximization) algorithm is used to 
remove the noise, with the integration of a CNN process as regularization, 
in order to create a new variational method. The method is able to iteratively 
estimate the noise parameters useful for classifying the type and level 
of noise in each pixel. The work carried out is divided into four phases: 
regularization, synthesis, parameter estimation and noise classification.

\subsection{RELATED WORK}
\subsubsection{Mixed Noise Model}
The mixed noise is:
\begin{equation}
    n = \left\{ \\
    \begin{array}{cc}
        n_1, & with~probability~r_1,\\
        n_2, & with~probability~r_2,\\
        \cdots, & \cdots\\
        n_k, & with~probability~r_k,\\ 
    \end{array}
     \right\}
\end{equation}
where $ n_k $ is the \emph{k}-th noise component with probability density function (PDF) 
$ p_k $ and $ r_k $ are the unknown mixture ratios and thei sum is equal to 1.

\subsubsection{Weighted K-SVD Model}
The following work is based on the method proposed in \cite{0884882814}. After obtaining 
the value of the mixed-noise \emph{Probability Density Function} (PDF), and 
assuming that the noise has a uniform distribution, we can derive the \emph{negative 
log-likelihood} function (\ref{PDF}) useful as {\bfseries{fidelity term}} in the variational 
method. 
\begin{equation}\label{PDF}
    \mathcal{L}(u,\Theta) = - \sum_{x\in\Omega}\ln\sum_{k=1}^Kr_kp_k(u(x)-f(x))
\end{equation}
In order for this probability to be extended to more types of noise 
distribution, it is necessary to perform an optimization that minimizes the 
upper limit of the function (\ref{H function}). 
\begin{eqnarray}\label{H function}
    \mathcal{H}(u,\Theta,w) & = & - \sum_{x\in\Omega}\sum_{k=1}\ln\left[r_kp_k(u(x)-f(x))\right]w_k \nonumber \\
                  &   & + \sum_{x\in\Omega}\sum_{k=1}w_k(x)\ln{w_k(x)}  
\end{eqnarray}
To minimize $\mathcal{H}$, a minimization of $ w $ must be performed, which is a probability 
indicates the noise at each pixel, and an iterative minimization of the other 
two variables $ u $ and $ \Theta $.

\subsubsection{IRCNN}
Another method \cite{0884882819} uses a CCN, based on the IRCNN model, to restore an 
image in which Gaussian noise is present. In this model, patches based $ L^2 $ 
residual loss functions are adopted. Unfortunately, if there are mixed-noises, 
this method does not produce satisfactory results.

\subsection{THE PROPOSED METHOD (EM-CNN)}
In the following paper a general variational model is proposed, formed by 
the combination of a deep learning method and variational methods, with 
regularized CNN to remove the mixed-noise problem, especially noises such 
as the Gaussian mixture and the Gaussian-impulse.

\subsubsection{General Model}
Removal of mixed-noise can be done with the following model:
\begin{equation}\label{HFunction}
    (u^*, \Theta^*, w^*) = \argmin_x\left\{H(u,\Theta,w) + \lambda_1\mathcal{J}(u)\right\}
\end{equation}
where $\mathcal{J}$ is a regularizer, such \emph{Total Variation Regularization} (TV) while $ \lambda_1 $ 
control the balance of the terms. By changing $u$ with an auxiliary function 
$v$, and applying the \emph{augmented Lagrangian method} (ALM) \cite{0884882833}, the minimizations 
on each component of $\mathcal{H}$ ($u$, $\Theta$, $w$) change. The resolution of these 
minimizations allows to divide the mixed-noise removal problem into Gaussian 
noise removal (renewing $u$), choice of the fidelity term (renewing $v$), 
noise reduction (updating the Lagragian multiplier $u$), parameter estimation 
noise ($\Theta$ update) and noise classification ($w$ update). The formula used by 
the model is able to handle different types of Gaussian noises such as impulse 
noise, mixed-noise Gaussian-impulse and many others. Each noise has 
a certain probability function pk, so just change this value in (\ref{H function}). A CNN 
can be used to learn Gaussian noise for different types of levels. In this paper 
the CNN based denoiser \cite{0884882819} was used. The weight $w$ is useful for assigning 
different fidelity terms to pixels contaminated by different levels of types of 
noise. The minimization of $\Theta$ and $w$ are part of EM processes. Thanks to 
these it is possible to extract information such as noise variations and the 
classification of this in different classes. In summary, the minimizations to 
be calculated are the following:

\begin{equation}\label{uMin}
    u^{v+1} = \argmin_u \lambda_1\mathcal{J}(u)+\mathcal{H}(u,\Theta^v,w^v),
\end{equation}
\begin{equation}\label{ThetaMin}
    \Theta^{v+1} = \argmin_\Theta \mathcal{H}(u^{v+1}, \Theta, w^v),
\end{equation}
\begin{equation}\label{wMin}
    w^{v+1} = \argmin_{w\in{S}} \mathcal{H}(u^{v+1}, \Theta^{v+1},w)
\end{equation}

\subsubsection{Gaussian Mixture Model}
A Gaussian mixture model is a probabilistic model that assumes all the data 
points are generated from a mixture og finite number of Gaussian distributions 
with unknown parameters. The distributions of Gaussian noises change 
according to the assumed variance ($\sigma^2$) value. The new function $ \mathcal{H} $ will be 
as follows:
\begin{eqnarray}\label{HFunctionGMM}
    \mathcal{H}(u,r,\sigma^2,w) & = & \frac{1}{2}\sum_{x\in\Omega}\sum_{k=1}^K\frac{(u(x)-f(x))^2}{\sigma^2_k}w_k(x)-\sum_{x\in\Omega}\sum_{k=1}^Kw_k(x)\ln{r_k}\nonumber \\
                                &   & + \frac{1}{2}\sum_{x\in\Omega}\sum_{k=1}^Kw_k(x)\ln{\sigma^2_k}+\frac{1}{2}\sum_{x\in\Omega}\sum_{k=1}^Kw_k(x)\ln{w_k}(x)
\end{eqnarray}
The optimizations (\ref{uMin}) (\ref{ThetaMin}) (\ref{wMin}) of the parameters of (\ref{HFunctionGMM}) are adapted by 
including the two new parameters $r$ and $\sigma^2$, enclosed in $\Theta$, which will be minimized 
in turn. Through the optimization of these parameters it is possible to remove 
the mixed-noise Gaussian. In summary, all the steps carried out are 
illustrated in figure \ref{fig:EM-CNN}.
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.7 \linewidth]{images/paper7/flowchart.png}
    \centering
    \caption{The flowchart of EM-CNN algorithm.}
    \label{fig:EM-CNN}
\end{figure}
Image degradation occurs using the additive noise model: 
$$ f = u + n $$
where $f$ is the observed image (the one resulting from the combination of 
noises), $u$ is the true image and $n$ is the introduced noise. The image $f$ will 
be the input of the CNN network which, through its denoise \cite{0884882819}, will perform 
a smoothing on the image obtaining a new image $u^{v+1}$. CNN applies two 
operations on this image in parallel. The first is that of the noise put back 
in which the noise present in the image f is extracted by simply subtracting 
the image $u^{v+1}$. Subsequently, to calcutale the fidelity term, the EM algorithm 
\cite{0884882814} is applied to this noise which has the task of estimating the noise 
and classifying it by minimizing the terms of \ref{HFunctionGMM}. The minimization of each 
weight $w$ leads to a continuous updating of this parameter which translates 
into greater precision in determining the noise on each individual pixel. The 
estimation of the parameters allows to carry out the synthesis process, or to 
select a new fidelity term, for each pixel, based on the estimates made on the 
noise. The second operation is to apply the \emph{Total Variation Regularization} 
(TV) \cite{0884882802} on the starting image $u^{v+1}$ useful for carrying out a good 
restructuring of the image from noise and blurred data. The image $V^v$ with the 
updated w weights, will be the new input of the CNN network that through 
multiple iterative processes will improve until its convergence, or until the 
fidelity term will no longer change. The output returned by the model will 
consist of a restructured image ($u^*$) accompanied by the estimates of the 
individual noises ($n_1^ *, n_2^*$) applied at the beginning.

\subsubsection{Gaussian Noise Plus Impulse Noise}
The overall noise can consist of several noise distributions. Previously, the 
operations useful for obtaining information on a Gaussian noise distribution 
have been defined. When the Gaussian distribution assumes random values 
(called salt and pepper), and therefore not uniformly distributed, then the 
optimization problem becomes more difficult. In order to find a solution, it 
is necessary to approximate a part of the PDF concerning the random-value 
noise by a Gaussian function, which means that it is possible to use the 
Gaussian mixture noise model to optimize the Gaussian plus impulse noise.

\subsection{EXPERIMENTAL RESULTS}
The experiments are conducted on mixed-noise Gaussian noise and on Gaussian 
plus impulse noise. The indexs used to estimate the quality of the restructured
images are the \emph{Signal-to-Noise Ratio} (PSNR) and the \emph{Structural 
Similarity Index} (SSIM) \cite{0884882841}.

\subsubsection{Gaussian Mixed Noise}
The quality comparison is done with different methods already existing in 
the state of the art. From figure \ref{fig:GMNComp} you can see the application of 
each of these methods on an image in which there is the presence of mixed-noise. 
The proposed method produces a restructured image with a better level of 
texture detail.
\begin{figure}[h!]
    \centering
    \includegraphics[width = \linewidth]{images/paper7/GMN comparison.png}
    \centering
    \caption{Image reconstruction under Gaussain mixture noise comparison.}
    \label{fig:GMNComp}
\end{figure}

To test the efficiency of the methods, several images belonging to a 
BSD100 dataset \cite{0884882842} were taken on which there are two Gaussian noises ($n_1$ 
and $n_2$) with different ratios $r_1, r_2$ and standard deviation ($\sigma$). By comparing the 
results obtained in table \ref{indexCompare}, the proposed method achieves the highest values 
in both indices compared to the two methods \cite{0884882815} \cite{0884882819} most common in the 
state of the art.
\begin{table}[h!]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c||ccc|ccc|ccc|}
        \hline
            & \multicolumn{3}{c||}{$\sigma_1=5 ~~~\sigma_2=30 $} & \multicolumn{3}{c||}{$\sigma_1=10 ~~~\sigma_2=50 $} & \multicolumn{3}{c||}{$\sigma_1=15 ~~~\sigma_2=75 $}\\
        \hline        
        $r_1:r_2\rightarrow$ & 0.3:0.7 & 0.5:0.5 & 0.7:0.3 & 0.3:0.7 & 0.5:0.5 & 0.7:0.3 & 0.3:0.7 & 0.5:0.5 & 0.7:0.3\\
        \hline 
        \hline
        \multirow{2}{*}{PGPD\cite{0884882815}} & 29.42 & 30.16 & 31.19 & 27.23 & 27.88 & 28.78 &25.64 & 26.26 & 27.14\\
        & 0.8132 & 0.8349 & 0.8544 & 0.7404 & 0.7634 & 0.7874 & 0.6790 & 0.7044 & 0.7336\\
        \hline
        \multirow{2}{*}{IRCNN\cite{0884882819}} & 29.83 & 30.53 & 31.48 & 27.66 & 28.25 & 29.04 & 19.37 & 25.22 & 27.32\\
        & 0.8262 & 0.8451 & 0.8664 & 0.7611 & 0.7789 & 0.7993 & 0.2948 & 0.6423 & 0.7426\\
        \hline
        \multirow{2}{*}{Proposed} & \bfseries{29.88} & \bfseries{31.26} & \bfseries{32.72} & \bfseries{28.01} & \bfseries{29.00} & \bfseries{30.37} & \bfseries{26.23} & \bfseries{27.41} & \bfseries{28.84}\\
        & \bfseries{0.8273} & \bfseries{0.8785} & \bfseries{0.9094} & \bfseries{0.7801} & \bfseries{0.8112} & \bfseries{0.8528} & \bfseries{0.6931} & \bfseries{0.7613} & \bfseries{0.8077}\\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Average PSNR and SSIM values on BSD100 datasets of some methods.}
    \label{indexCompare}
\end{table}

\subsubsection{Gaussian Noise Plus Impulse Noise}
To remove the “salt and pepper” noise, only one image was used on which 
this type of noise was applied. Also for this type of noise, the evaluation is 
made by changing the mixture ratio r and the sigma standard deviation with 
different values. The work done by the proposed method, compared with the 
other methods, is visible in figure \ref{fig:saltComp}. As for the values obtained of the PSNR 
and SSIM indexs, these are visible in table \ref{GNPINIndex}.
\begin{figure}[h!]
    \centering
    \includegraphics[width = \linewidth]{images/paper7/salt.png}
    \centering
    \caption{Image reconstruction under Gaussain noise plus random-valued noise comparison.}
    \label{fig:saltComp}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c||ccc|ccc|ccc|}
        \hline
            & \multicolumn{3}{c||}{$\sigma_1=5$} & \multicolumn{3}{c||}{$\sigma_1=10$} & \multicolumn{3}{c||}{$\sigma_1=15$}\\
        \hline        
        $r\rightarrow$ & 0.1 & 0.2 & 0.3 & 0.1 & 0.2 & 0.3 & 0.1 & 0.2 & 0.3\\
        \hline 
        \hline
        \multirow{2}{*}{Noisy} & 18.76 & 15.76 & 14.04 & 18.43 & 15.61 & 13.95 & 17.94 & 15.38 & 13.81\\
        & 0.3545 & 0.2290 & 0.1662 & 0.3313 & 0.2216 & 0.1626 & 0.3040 & 0.2109 & 0.1571\\
        \hline
        \multirow{2}{*}{Two-phase\cite{0884882828}} & 25.40 & 24.77 & 24.13 & 24.34 & 23.94 & 23.45 & 23.32 & 23.02 & 22.67\\
        & 0.6599 & 0.6313 & 0.5957 & 0.5116 & 0.4914 & 0.4655 & 0.4224 & 0.4058 & 0.3854\\
        \hline
        \multirow{2}{*}{ACWMF+K-SVD \cite{0884882844}\cite{0884882813}} & 26.07 & 25.27 & 24.54 & 25.50 & 24.91 & 23.13 & 24.64 & 24.19 & 23.67\\
        & 0.7130 & 0.6761 & 0.6333 & 0.5902 & 0.5613 & 0.5291 & 0.4855 & 0.4625 & 0.4414\\
        \hline    
        \multirow{2}{*}{LSM-NLR \cite{0884882826}} & 29.48 & 26.97 & 24.55 & 29.43 & 27.08 & 24.43 & 28.51 & 26.06 & 23.98\\
        & 0.6548 & 0.5774 & 0.4982 & 0.6420 & 0.5646 & 0.4770 & 0.6282 & 0.5458 & 0.4643\\
        \hline
        \multirow{2}{*}{$l_1-l_0$\cite{0884882829}} & 30.45 & 27.75 & 25.95 & 28.45 & 26.59 & 25.34 & 27.33 & 25.69 & 24.55\\
        & 0.8962 & 0.7911 & 0.7312 & 0.7539 & 0.6748 & 0.6293 & 0.6793 & 0.6010 & 0.5744\\
        \hline
        \multirow{2}{*}{Proposed} & \bfseries{31.06} & \bfseries{30.44} & \bfseries{27.66} & \bfseries{30.97} & \bfseries{29.5} & \bfseries{27.41} & \bfseries{29.74} & \bfseries{28.09} & \bfseries{25.73}\\
        & \bfseries{0.9214} & \bfseries{0.9157} & \bfseries{0.8497} & \bfseries{0.8943} & \bfseries{0.8749} & \bfseries{0.8316} & \bfseries{0.8530} & \bfseries{0.8222} & \bfseries{0.7750}\\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Average PSNR and SSIM values on one image of some methods.}
    \label{GNPINIndex}
\end{table}

\subsubsection{Realistic Noise Removal}
The model was also tested with the images present in the dataset provided 
in \cite{0884882845}. From the following images only one of their cut out parts is taken 
(Fig \ref{fig:crop}). This time the EM-CNN model was compared with different types 
of existing denoising, including the IRCNN which is adopted by CNN of the 
proposed model. Figure \ref{fig:cropRes} shows the results, in terms of PSNR, obtained on 
each cropped image. From these results it can be seen that the proposed 
model does not always manage to obtain the best result, but in any case its 
PSNR average remains higher than the competing denoisers.
\begin{figure}[h!]
    \centering
    \includegraphics[width = \linewidth]{images/paper7/crop.png}
    \centering
    \caption{Cropped real noisy images.}
    \label{fig:crop}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.7\linewidth]{images/paper7/crop result.png}
    \centering
    \caption{PSNR result of different methods on cropped real noisy images.}
    \label{fig:cropRes}
\end{figure}

\subsection{CONCLUSION}
Noise removal, as well as being carried out by the denoiser, is given by the 
learning made by the CNN network on the new input images which, through 
an iterative process, are gradually improved. This is possible only thanks to 
the estimates derived from the EM algorithm and how this can classify the 
various types of noise. Without this information, each input image would 
not get the proper correction, thus generating incorrect output images.